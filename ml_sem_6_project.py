# -*- coding: utf-8 -*-
"""ML sem 6 project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IkZS6k2PFhtV_p3T1CZyPvZJPbqX0kAb
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

"""# 1. Read Dataset from UCI machine learning repository"""

df = pd.read_csv("/content/tic_2000_eval_data.csv")
df.head()

df.info()

df.isnull().sum()

df.describe()

"""# 2. Apply Data Pre processing
# Drop unnecessary columns
"""

# Renaming columns
df.columns = ["Attribute " + str(i) for i in range(1, len(df.columns)+1)]

# Dropping redundant columns
df = df.drop(["Attribute 37", "Attribute 38"], axis=1)

# Checking for missing values
print(df.isnull().sum())

# Converting categorical variables to numerical variables
cat_vars = ["Attribute 2", "Attribute 7", "Attribute 8", "Attribute 9", "Attribute 10", 
            "Attribute 11", "Attribute 17", "Attribute 18", "Attribute 19", "Attribute 20", 
            "Attribute 21", "Attribute 22", "Attribute 23", "Attribute 24", "Attribute 25", 
            "Attribute 26", "Attribute 27", "Attribute 28", "Attribute 29", "Attribute 30", 
            "Attribute 31", "Attribute 32", "Attribute 33", "Attribute 34", "Attribute 35"]
df[cat_vars] = df[cat_vars].apply(lambda x: pd.factorize(x)[0])

"""#Applying classification and regression models

"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

X = df.drop(["Attribute 1"], axis=1)
y = df["Attribute 1"]

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#REGRESSION MODELS

#1.Linear Regression
"""

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred_lin_reg = lin_reg.predict(X_test)
mse_lin_reg = mean_squared_error(y_test, y_pred_lin_reg)
print("Linear Regression MSE:", mse_lin_reg)

"""
#2.Decision Tree Regressor"""

dt_reg = DecisionTreeRegressor()
dt_reg.fit(X_train, y_train)
y_pred_dt_reg = dt_reg.predict(X_test)
mse_dt_reg = mean_squared_error(y_test, y_pred_dt_reg)
print("Decision Tree Regressor MSE:", mse_dt_reg)

"""#3.Random forest regressor"""

rf_reg = RandomForestRegressor()
rf_reg.fit(X_train, y_train)
y_pred_rf_reg = rf_reg.predict(X_test)
mse_rf_reg = mean_squared_error(y_test, y_pred_rf_reg)
print("Random Forest Regressor MSE:", mse_rf_reg)

"""#4.Support Vector Regressor"""

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

# Create SVR model
svr_model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)

# Train SVR model
svr_model.fit(X_train, y_train)

# Predict using SVR model
y_pred_svr = svr_model.predict(X_test)

# Evaluate SVR model
mse_svr = mean_squared_error(y_test, y_pred_svr)
print("Mean Squared Error of SVR model:", mse_svr)

"""#5.Gradient Boosting Regressor:"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Create Gradient Boosting model
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Train Gradient Boosting model
gb_model.fit(X_train, y_train)

# Predict using Gradient Boosting model
y_pred_gb = gb_model.predict(X_test)

# Evaluate Gradient Boosting model
mse_gb = mean_squared_error(y_test, y_pred_gb)
print("Mean Squared Error of Gradient Boosting model:", mse_gb)

"""#CLASSIFICATION MODELS

#1.Logistic Regression
"""

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)
accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)
print("Logistic Regression Accuracy:", accuracy_log_reg)

"""
#2.Decision Tree Classifier"""

dt_class = DecisionTreeClassifier()
dt_class.fit(X_train, y_train)
y_pred_dt_class = dt_class.predict(X_test)
accuracy_dt_class = accuracy_score(y_test, y_pred_dt_class)
print("Decision Tree Classifier Accuracy:", accuracy_dt_class)

"""#3.Random Forest Classifier"""

rf_class = RandomForestClassifier()
rf_class.fit(X_train, y_train)
y_pred_rf_class = rf_class.predict(X_test)
accuracy_rf_class = accuracy_score(y_test, y_pred_rf_class)
print("Random Forest Classifier Accuracy:", accuracy_rf_class)

"""#4.SUPPORT VECTOR MACHINES(SVM)"""

from sklearn import svm
from sklearn.metrics import accuracy_score

# Create SVM classifier
svm_classifier = svm.SVC(kernel='linear', C=1, random_state=42)

# Train SVM classifier
svm_classifier.fit(X_train, y_train)

# Predict using SVM classifier
y_pred_svm = svm_classifier.predict(X_test)

# Evaluate SVM classifier
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print("Accuracy of SVM classifier:", accuracy_svm)

"""#5.NAIVE BAYES CLASSIFIER"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Create Naive Bayes classifier
nb_classifier = GaussianNB()

# Train Naive Bayes classifier
nb_classifier.fit(X_train, y_train)

# Predict using Naive Bayes classifier
y_pred_nb = nb_classifier.predict(X_test)

# Evaluate Naive Bayes classifier
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print("Accuracy of Naive Bayes classifier:", accuracy_nb)

"""#Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV

"""
 Tuning hyperparameters for Decision Tree Regressor"""

param_grid_dt_reg = {
    "max_depth": [3, 5, 10],
    "min_samples_leaf": [2, 5, 10],
    "max_features": ["sqrt", "log2"]
}
grid_search_dt_reg = GridSearchCV(estimator=dt_reg, param_grid=param_grid_dt_reg, 
                                  cv=5, n_jobs=-1, verbose=2)
grid_search_dt_reg.fit(X_train, y_train)
print("Best Parameters for Decision Tree Regressor:", grid_search_dt_reg.best_params_)

""" hyperparameterTuning for Logistic Regression

---


"""

param_grid_log_reg = {
    "penalty": ["l1", "l2"],
    "C": [0.01, 0.1, 1, 10]
}
grid_search_log_reg = GridSearchCV(estimator=log_reg, param_grid=param_grid_log_reg, 
                                   cv=5, n_jobs=-1, verbose=2)
grid_search_log_reg.fit(X_train, y_train)
print("Best Parameters for Logistic Regression:", grid_search_log_reg.best_params_)

"""Huperparametertuning for Random Forest Regressor"""

param_grid_rf_reg = {
    "n_estimators": [100, 500, 1000],
    "max_depth": [3, 5, 10],
    "min_samples_leaf": [2, 5, 10],
    "max_features": ["sqrt", "log2"]
}
grid_search_rf_reg = GridSearchCV(estimator=rf_reg, param_grid=param_grid_rf_reg, 
                                  cv=5, n_jobs=-1, verbose=2)
grid_search_rf_reg.fit(X_train, y_train)
print("Best Parameters for Random Forest")

"""#R-SQUARED SCORE"""

from sklearn.metrics import r2_score

"""#1.FOR SVR"""

r2_svr = r2_score(y_test, y_pred_svr)
print("R-squared score of SVR model:", r2_svr)

"""#2.for Gradient Boosting model"""

r2_gb = r2_score(y_test, y_pred_gb)
print("R-squared score of Gradient Boosting model:", r2_gb)

"""#3.for Linear Regression model"""

r2_lin_reg = r2_score(y_test, y_pred_lin_reg)
print("R-squared score of Linear Regression model:", r2_lin_reg)

"""#4.Decision Tree Regressor model"""

r2_reg = r2_score(y_test, y_pred_reg)
print("R-squared score of Decision Tree Regressor model:", r2_reg)

"""#5.Random Forest Regressor model"""

r2_rfr = r2_score(y_test, y_pred_rfr)
print("R-squared score of Random Forest Regressor model:", r2_rfr)

"""#EDA"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
files = files.upload()

df = pd.read_csv("tic_2000_eval_data.csv")

print(df.head())

print(df.info())

"""# Check summary statistics"""

print(df.describe())

df = pd.read_csv("tic_2000_target_data.csv")

print(df.head())

print(df.describe())

"""# Check correlation between features"""

corr_matrix = df.corr()
sns.heatmap(corr_matrix, cmap="YlGnBu")
plt.title("Correlation Matrix")
plt.show()

"""# Check distribution of target variable"""

sns.histplot(df["Target"], kde=True)
plt.title("Distribution of Target Variable")
plt.show()

"""# Check distribution of numerical features"""

num_cols = df.select_dtypes(include=["float64", "int64"]).columns
df[num_cols].hist(figsize=(12, 6))
plt.suptitle("Distribution of Numerical Features", y=1.05, fontsize=16)
plt.tight_layout()
plt.show()

"""# Check relationship between features and target variable"""

sns.pairplot(df, x_vars=num_cols[:-1], y_vars=["MOSTYPE"], kind="reg", height=3)
plt.suptitle("Relationship between Features and Target Variable", y=1.05, fontsize=16)
plt.tight_layout()
plt.show()

"""#HEAT MAP"""

import seaborn as sns

# Create correlation matrix
corr = df.corr()

# Create heatmap
sns.heatmap(corr, cmap="YlGnBu")

"""#cross-validation"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
import numpy as np

# Load data
data = pd.read_csv("tic_2000_eval_data.csv")

X = data.drop(["MOSTYPE"], axis=1)
y = data["MOSTYPE"]

# Create linear regression model
lin_reg = LinearRegression()

# Perform cross validation
scores = cross_val_score(lin_reg, X, y, cv=5, scoring='neg_mean_squared_error')

# Print mean and standard deviation of cross validation scores
print("Cross validation scores:", np.sqrt(-scores))
print("Mean RMSE:", np.mean(np.sqrt(-scores)))
print("Standard deviation of RMSE:", np.std(np.sqrt(-scores)))

"""#RANSAC """

import pandas as pd

from google.colab import files
files=files.upload()

df = pd.read_csv("tic_2000_eval_data.csv")

df.head()

print(df)

X=df['MOSTYPE']
Y=df['MGEMOMV']
from sklearn.linear_model import RANSACRegressor
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt
xtrain,xtest,ytrain,ytest=train_test_split(X,Y,test_size=0.3)

ran_reg=RANSACRegressor(LinearRegression())
ran_reg.fit(xtrain.values.reshape(-1,1),ytrain)

inliers=ran_reg.inlier_mask_
outliers=np.logical_not(inliers)

plt.scatter(xtrain.values.reshape(-1,1)[inliers],ytrain[inliers],color="green")
plt.scatter(xtrain.values.reshape(-1,1)[outliers],ytrain[outliers],color="red")
plt.plot(xtrain,ran_reg.predict(xtrain.values.reshape(-1,1)))
plt.show()

"""#POLYNOMIAL REGRESSION"""

from sklearn.preprocessing import PolynomialFeatures

data=pd.read_csv("tic_2000_eval_data.csv")
plt.scatter(data['MGODRK'],data['MOSTYPE'])
plt.show()

